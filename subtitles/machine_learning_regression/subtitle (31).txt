[MUSIC] So, to start with,
let's look at the machine learning model. This multiple regression model. And let's just recall where we left off
in the last module, where we were talking about simple linear regression, where our
goal was just to fit a line to the data. And we just had a single input. And in our example, we always
talked about having square feet and trying to model the relationship
between square feet of a house and the output,
which was the value of the house. But as the name implies, this simple
linear regression model is really simple. And in a lot of cases, we're gonna be interested in more
complex functions of our input. So one example of this is something
called polynomial regression. And we actually saw this back in
the first course in the specialization. And in that case, what we did was we took our simple linear
regression model and our fit of that. Of course at that time, when we were in
the first course of the specialization, we didn't have all this terminology
that we learned in the last module, but now we know that this is
a simple linear regression model. And we take this fit and we show it
to our friend and we say, hey, look, this is so cool. I have this line that I fit to my data,
and now I can predict the value of my house. And your friend is a little
bit skeptical and says, dude, it's not a linear relationship between
square feet and the value of a house. He's looking at the data and
saying he just doesn't believe it. Instead, he thinks it's a quadratic fit. So, what your friend is saying is that he
doesn't believe the model that you use. He doesn't believe that it's just this
linear relationship, of course plus error. He thinks that there's this quadratic
function, which has the following equation, underlying the relationship
between square feet and house value. And again, our regression model is gonna assume
that there's some noise around that. But of course, you could consider
even higher order polynomials. For example, here, I'm showing some pth
order polynomial that you might choose to be your model of the relationship between
square feet and the value of the house. So here's our generic
polynomial regression model, where we take our observation, yi, and
model it as this polynomial in terms of, for example, square feet of our house,
which is just some input x. And then we assume that
there's some error, epsilon i. So that's the error associated
with the ith observation. And what we see is that in this model, in
contrast to our simple linear regression model, we have all these powers of x
that are now appearing in the model. And what we can do, is we can treat
these different powers of x as features. Okay, so now we're introducing
this new word features. And what features are,
they're just some function of your input. So, in this case, in particular,
our features, just to be very explicit, our first feature of the model
is just the number 1, which is called the constant feature. Then the second feature of our model is x. So, that's just the linear term, just
like we had in simple linear regression. And then our third feature is x squared. And we keep going up to our p+1 feature,
which is x to the power p. And associated with each one of these
features in our model is a parameter. So we have some p+1 parameters, w0,
which is just the intercept term. All the way up to wp, the coefficient
associated with pth power of our input. [MUSIC]